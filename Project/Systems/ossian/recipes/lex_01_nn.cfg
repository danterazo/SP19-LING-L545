#!/usr/bin/env python

import sys
import os
import inspect
current_dir = os.path.realpath(os.path.abspath(os.path.dirname(inspect.getfile(inspect.currentframe()))))

## for when config is still in recipes directory:
sys.path.append(current_dir + '/../scripts/')
sys.path.append(current_dir + '/../scripts/processors/')

## for after config is copied to voice.cfg:
sys.path.append(current_dir + '/../../../../scripts/')
sys.path.append(current_dir + '/../../../../scripts/processors/')

from Tokenisers import RegexTokeniser
#from Phonetisers import NaivePhonetiser
from Lexicon import Lexicon
from FeatureExtractor import WorldExtractor
from FeatureDumper import FeatureDumper
from Aligner import StateAligner
from SKLProcessors import SKLDecisionTreePausePredictor 
from PhraseMaker import PhraseMaker
from AcousticModel import AcousticModelWorld
from NN import NNDurationPredictor, NNAcousticPredictor


import default.const as c







## ----------------------------------------------------------------
## First define a few things used later:

## Some useful Xpaths and regex:--
#CONTENT_NODES = "//token[@token_class='word'] | //token[@token_class='punctuation']"
JUNCTURE_NODES = "//token[@token_class='space'] | //token[@token_class='punctuation']"

LETTER_PATT = '[\p{L}||\p{N}||\p{M}]'
PUNC_PATT = '[\p{C}||\p{P}||\p{S}]'
SPACE_PATT = '\p{Z}'
PUNC_OR_SPACE_PATT = '[\p{Z}||\p{C}||\p{P}||\p{S}]'

## 
speech_coding_config = {'order': 59, 'static_window': '1', 'delta_window': '-0.5 0.0 0.5', 'delta_delta_window': '1.0 -2.0 1.0'}


pause_predictor_features = [        
        ('response', './attribute::has_silence="yes"'), 
        ('token_is_punctuation', './attribute::token_class="punctuation"'),
        ('end_of_sentence', './attribute::token_class="%s"'%(c.TERMINAL)),        
        ('since_start_utterance_in_words', "count(preceding::token[@token_class='word'])"),
        ('till_end_utterance_in_words', "count(following::token[@token_class='word'])")
]     


## The following work at both state and phone level:
phone_and_state_contexts = [

   ## SYLLABLE LEVEL:
    ("length_left_syllable", "count(ancestor::syllable/preceding::syllable[1]/descendant::segment)"),
    ("length_current_syllable", "count(ancestor::syllable/descendant::segment)"),
    ("length_right_syllable", "count(ancestor::syllable/following::syllable[1]/descendant::segment)"),
    ("since_beginning_of_syllable", "count_Xs_since_start_Y('segment', 'syllable')"),
    ("till_end_of_syllable", "count_Xs_till_end_Y('segment', 'syllable')"),

    ## WORD LEVEL:
    ("length_left_word_in_segments", "count(ancestor::token/preceding::token[@token_class='word'][1]/descendant::segment)"),
    ("length_current_word_in_segments", "count(ancestor::token/descendant::segment)"),
    ("length_right_word_in_segments", "count(ancestor::token/following::token[@token_class='word'][1]/descendant::segment)"),
    ("since_beginning_of_word_in_segments", "count_Xs_since_start_Y('segment', 'token')"),
    ("till_end_of_word_in_segments", "count_Xs_till_end_Y('segment', 'token')"),

    ("length_left_word_in_syllables", "count(ancestor::token/preceding::token[@token_class='word'][1]/descendant::syllable)"),
    ("length_current_word_in_syllables", "count(ancestor::token/descendant::syllable)"),
    ("length_right_word_in_syllables", "count(ancestor::token/following::token[@token_class='word'][1]/descendant::syllable)"),
    ("since_beginning_of_word_in_syllables", "count_Xs_since_start_Y('syllable', 'token')"),
    ("till_end_of_word_in_syllables", "count_Xs_till_end_Y('syllable', 'token')"),

    ## phrase LEVEL:
    ("length_l_phrase_in_words", "count(ancestor::phrase/preceding::phrase[1]/descendant::token[@token_class='word'])"),
    ("length_c_phrase_in_words", "count(ancestor::phrase/descendant::token[@token_class='word'])"),
    ("length_r_phrase_in_words", "count(ancestor::phrase/following::phrase[1]/descendant::token[@token_class='word'])"),

    ("length_l_phrase_in_syllables", "count(ancestor::phrase/preceding::phrase[1]/descendant::syllable)"),
    ("length_c_phrase_in_syllables", "count(ancestor::phrase/descendant::syllable)"),
    ("length_r_phrase_in_syllables", "count(ancestor::phrase/following::phrase[1]/descendant::syllable)"),

    ("length_l_phrase_in_segments", "count(ancestor::phrase/preceding::phrase[1]/descendant::segment)"),
    ("length_c_phrase_in_segments", "count(ancestor::phrase/descendant::segment)"),
    ("length_r_phrase_in_segments", "count(ancestor::phrase/following::phrase[1]/descendant::segment)"),

    ("since_phrase_start_in_segs", "count_Xs_since_start_Y('segment', 'phrase')"),
    ("till_phrase_end_in_segs", "count_Xs_till_end_Y('segment', 'phrase')"),

    ("since_phrase_start_in_syllables", "count_Xs_since_start_Y('syllable', 'phrase')"),
    ("till_phrase_end_in_syllables", "count_Xs_till_end_Y('syllable', 'phrase')"),

    ("since_phrase_start_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'phrase')"),
    ("till_phrase_end_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'phrase')"),

    ## SENTENCE (UTT) LEVEL
    ("since_start_sentence_in_segments", "count_Xs_since_start_Y('segment', 'utt')"),
    ("since_start_sentence_in_syllables", "count_Xs_since_start_Y('syllable', 'utt')"),    
    ("since_start_sentence_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'utt')"),
    ("since_start_sentence_in_phrases", "count_Xs_since_start_Y('phrase', 'utt')"),
    ("till_end_sentence_in_segments", "count_Xs_till_end_Y('segment', 'utt')"),
    ("till_end_sentence_in_syllables", "count_Xs_till_end_Y('syllable', 'utt')"),    
    ("till_end_sentence_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'utt')"),
    ("till_end_sentence_in_phrases", "count_Xs_till_end_Y('phrase', 'utt')"),
    ("length_sentence_in_segments", "count(ancestor::utt/descendant::segment)"),
    ("length_sentence_in_syllables", "count(ancestor::utt/descendant::syllable)"),    
    ("length_sentence_in_words", "count(ancestor::utt/descendant::token[@token_class='word'])"),
    ("length_sentence_in_phrases", "count(ancestor::utt/descendant::phrase)")
]
    
phone_contexts = [
        ## special named features:
        ('htk_monophone', './attribute::pronunciation'),
        ('start_time', './attribute::start'),
        ('end_time', './attribute::end'),

        ## normal features:
        ('ll_segment', 'preceding::segment[2]/attribute::pronunciation'),
        ('l_segment', 'preceding::segment[1]/attribute::pronunciation'),
        ('c_segment', './attribute::pronunciation'),
        ('r_segment', 'following::segment[1]/attribute::pronunciation'),
        ('rr_segment', 'following::segment[2]/attribute::pronunciation'),
] + phone_and_state_contexts




state_contexts = [

    ## special named features:
    ("start_time", "./attribute::start"),
    ("end_time", "./attribute::end"),
    ("htk_state", "count(./preceding-sibling::state) + 1"),
    ("htk_monophone", "./ancestor::segment/attribute::pronunciation"),

    ("ll_segment", "./ancestor::segment/preceding::segment[2]/attribute::pronunciation"),
    ("l_segment", "./ancestor::segment/preceding::segment[1]/attribute::pronunciation"),
    ("c_segment", "./ancestor::segment/attribute::pronunciation"),
    ("r_segment", "./ancestor::segment/following::segment[1]/attribute::pronunciation"),
    ("rr_segment", "./ancestor::segment/following::segment[2]/attribute::pronunciation")
] + phone_and_state_contexts

duration_data_contexts = [('state_%s_nframes'%(i), '(./state[%s]/attribute::end - ./state[%s]/attribute::start) div 5'%(i,i)) for i in [1,2,3,4,5]]


## ----------------------------------------------------------------
## Now, a number of utterance processors are defined:--

tokeniser = RegexTokeniser('word_splitter', target_nodes='//utt', split_attribute='text', \
                            child_node_type='token', add_terminal_tokens=True, \
                            class_patterns = [('space', '\A'+SPACE_PATT+'+\Z'), ('punctuation', '\A'+PUNC_OR_SPACE_PATT+'+\Z')], \
                            split_pattern='('+SPACE_PATT+'*'+PUNC_PATT+'*'+SPACE_PATT+'+|'+SPACE_PATT+'*'+PUNC_PATT+'+\Z)'  )
                                                    ## modified to handle word-internal hyphens

phonetiser = Lexicon('segment_adder', target_nodes="//token", target_attribute='text', child_node_type='segment', \
                            class_attribute='token_class', output_attribute='pronunciation', word_classes = ['word'], \
                            probable_pause_classes = ['punctuation', c.TERMINAL], possible_pause_classes=['space'],\
                            dictionary='cmudict', lts_ntrain=1000, lts_gram_length=2)

speech_feature_extractor = WorldExtractor('acoustic_feature_extractor', input_filetype='wav', output_filetype='cmp', \
                            coding_config=speech_coding_config, sample_rate=48000, alpha=0.77, mcep_order=59)

align_label_dumper = FeatureDumper(target_nodes='//segment', output_filetype='align_lab', contexts=[('segment', './attribute::pronunciation')])
 
aligner = StateAligner(target_nodes='//segment', target_attribute='pronunciation', input_label_filetype='align_lab', acoustic_feature_filetype='cmp', \
                    output_label_filetype='time_lab', silence_tag='has_silence', min_silence_duration=50, viterbi_beam_width='1000 100000 1000000', \
                    acoustic_subrecipe='standard_alignment')

pause_predictor = SKLDecisionTreePausePredictor(processor_name='pause_predictor', target_nodes=JUNCTURE_NODES, output_attribute='silence_predicted', contexts=pause_predictor_features)
        
phrase_adder = PhraseMaker(node_type_to_regroup='token', parent_node_type='phrase', \
                 attribute_with_silence='pronunciation', silence_symbol='sil')

dur_data_maker = FeatureDumper(processor_name='duration_data_maker', target_nodes='//segment', context_separators='spaces', output_filetype='dur_data', \
                question_file='null', contexts=duration_data_contexts)

dur_label_maker = FeatureDumper(processor_name='labelmaker', target_nodes='//segment', context_separators='numbers', output_filetype='lab_dur', \
                question_file='questions_dur.hed', question_filter_threshold = 5, contexts=phone_contexts)

duration_predictor = NNDurationPredictor(processor_name='duration_predictor', question_file='questions_dur.hed', target_nodes='//segment')

dnn_label_maker = FeatureDumper(processor_name='dnn_labelmaker', target_nodes='//state', context_separators='numbers', output_filetype='lab_dnn', \
                question_file='questions_dnn.hed', question_filter_threshold=5, contexts=state_contexts)

acoustic_predictor = NNAcousticPredictor(variance_expansion=0.3, sample_rate=48000, alpha=0.77, mcep_order=59, input_label_filetype='lab_dnn')


## -----------------------------------------------------------------
## The processors are grouped for convenience into several 'stages':

text_proc = [tokeniser, phonetiser]
alignment = [align_label_dumper, speech_feature_extractor, aligner, pause_predictor, phrase_adder, dur_data_maker]
pause_prediction = [pause_predictor, phrase_adder] 
speech_generation = [dur_label_maker, duration_predictor, dnn_label_maker, acoustic_predictor]



## ----------------------------------------------------------------
## The final part of the config specifies which stages are run in each of the modes
## "train" and "runtime" (and optionally extra, specialised, modes):

train_stages   = [text_proc, alignment,        speech_generation]

runtime_stages = [text_proc, pause_prediction, speech_generation]

